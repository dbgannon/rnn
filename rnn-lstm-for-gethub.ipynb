{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a ulitity to pull the word from a line in the vocab file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelpath = \"path to your model directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pullword(l):\n",
    "    i = 0\n",
    "    while l[i] == ' ' or l[i] == '\\t': i+=1\n",
    "    while l[i] != ' ' and l[i] != '\\t': i+=1\n",
    "    while l[i] == ' ' or l[i] == '\\t':  i+=1\n",
    "    while l[i] != ' ' and l[i] != '\\t': i+=1\n",
    "    while l[i] == ' ' or l[i] == '\\t': i+= 1\n",
    "    strm = \"\"\n",
    "    while l[i] != ' ' and l[i] != '\\t':\n",
    "        strm = strm+l[i]\n",
    "        i += 1\n",
    "    return strm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenTensor is a function that opens the trained model files generated by cntk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opentensor(path):\n",
    "    with open(path) as file:\n",
    "        El = [[float(digit) for digit in line.split()] for line in file]\n",
    "    return np.array(El)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = opentensor(modelpath + '/E0.txt')\n",
    "bO = opentensor(modelpath + '/bo0.txt')\n",
    "WHO = opentensor(modelpath + '/WHO.txt')\n",
    "WCO = opentensor(modelpath + '/WCO0.txt')\n",
    "WXF = opentensor(modelpath + '/WXF.txt')\n",
    "bF = opentensor(modelpath + '/bf0.txt')\n",
    "WHF = opentensor(modelpath + '/WHF0.txt')\n",
    "WCF = opentensor(modelpath + '/WCF0.txt')\n",
    "WXI = opentensor(modelpath + '/WXIO.txt')\n",
    "WHI = opentensor(modelpath + '/WHI.txt')\n",
    "WCI = opentensor(modelpath + '/WCIO.txt')\n",
    "WXC = opentensor(modelpath + '/WXC.txt')\n",
    "WXO = opentensor(modelpath + '/WXO.txt')\n",
    "WHC = opentensor(modelpath + '/WHC0.txt')\n",
    "bC = opentensor(modelpath + '/bc0.txt')\n",
    "bI = opentensor(modelpath + '/bi0.txt')\n",
    "W2 = opentensor(modelpath + '/W2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next open the vocabulary file and create a list of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordlines = [line.rstrip('\\n') for line in open(modelpath + '/vocab.txt', \"rb\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for l in wordlines:\n",
    "    wordlist.extend([pullword(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worddict = { wordlist[i]: i for i in range(len(wordlist))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary is size 10000 and E is a 150x10000 matrix that has learned the compact representation of each word.\n",
    "getvec takes an english word looks in the wordlist to see if is there.   If so, it returns the corresponding\n",
    "column vector of lenght 150. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getvec(word, E):\n",
    "    try:\n",
    "        ind = worddict[word]\n",
    "    except:\n",
    "        print \"word \" + word + \" not in dictionary\"\n",
    "        return\n",
    "    V = E[:,ind]\n",
    "    V.shape = (150,1)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output vector of the rnn is a vector of length 10000.   output[i] represents the relative likelyhood that the next word is the best to follow the string so far.  Getwordsfromoutput returns the top 5 candidate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getwordsfromoutput(output):\n",
    "    lst = []\n",
    "    for i in range(10000):\n",
    "        lst.extend([(output[0,i], i)])\n",
    "    dotsl = sorted(lst, key=lambda tup: -tup[0])\n",
    "    #print dotsl[0:5]\n",
    "    st = []\n",
    "    for i in range(5):\n",
    "        st.extend([wordlist[dotsl[i][1]]])\n",
    "    return st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnn is a direct translation of the lstm equations.  the only difference is that we use an english word as input an return a list five possible nextwords as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(word,old_h, old_c):\n",
    "      #features = SparseInputValue -> [10000 x *]\n",
    "      Xvec = getvec(word, E)\n",
    "\n",
    "      i = Sigmoid(np.matmul(WXI, Xvec) + np.matmul(WHI, old_h) + WCI * old_c + bI)\n",
    "      f = Sigmoid(np.matmul(WXF, Xvec) + np.matmul(WHF, old_h) + WCF * old_c + bF)\n",
    "      \n",
    "      c = f*old_c + i *(np.tanh(np.matmul(WXC, Xvec) + np.matmul(WHC, old_h) + bC))\n",
    "      \n",
    "      o = Sigmoid(np.matmul(WXO, Xvec)+ np.matmul(WHO, old_h)+ (WCO * c)+ bO)\n",
    "      \n",
    "      h = o * np.tanh(c)\n",
    "      \n",
    "      #extract ordered list of five best possible next words\n",
    "      q = h.copy()\n",
    "      q.shape = (1, 200)\n",
    "      output = np.matmul(q, W2)\n",
    "      outlist = getwordsfromoutput(output)\n",
    "      return h, c, outlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This takes any word as a starting point and constructs a sentence that is defined by the sequence generated by the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the next word we randomly pick one of the top three suggested by the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the $ 300-a-share offer for $ N million or five times a share last year when terms futures plunged slightly sharply against the u.s. currency because there still saw evidence before interest expense dropped sharply above friday afternoon partly due mostly during october when stock-index prices are rising about a$ before stock-index funds which buy dividends associated over stock-index arbitrage trades following today following this session despite a continuing market price trends primarily because its stock tumbled slightly higher than usual because lin might soon have received orders through underwriters at kidder peabody corp </s> morgan holdings ltd which holds three.\n"
     ]
    }
   ],
   "source": [
    "c = np.zeros(shape = (200, 1))\n",
    "h = np.zeros(shape = (200, 1))\n",
    "output = np.zeros(shape = (10000, 1))\n",
    "word = 'the'\n",
    "sentence= word \n",
    "for _ in range(100):\n",
    "    h, c, outlist = rnn(word, h, c)\n",
    "    word = outlist[randint(0,4)]\n",
    "    sentence = sentence + \" \" +word\n",
    "\n",
    "print sentence+\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
