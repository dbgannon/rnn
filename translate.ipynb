{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Notebook version of the translate.py program from Tensorflow\n",
    "It does not do the training but it is designed to allow you to interactivly poke and prod the model.  To use this you need to run the tranlate.py program with\n",
    "\n",
    "python translate.py\n",
    "  --data_dir [your_data_directory] --train_dir [checkpoints_directory]\n",
    "  --size=256 --num_layers=2 --steps_per_checkpoint=50\n",
    "  \n",
    "then set the paths to your_data_directory and the checkpoints_directory in mydata_dir and my_traindir variables below.\n",
    "\n",
    "this program is effectively the same as running translate.py in decode mode.\n",
    "\n",
    "python translate.py --decode\n",
    "  --data_dir [your_data_directory] --train_dir [checkpoints_directory]\n",
    "\n",
    "\n",
    "See the copyright for translate.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "#for these make sure PYTHONPATH is set.\n",
    "import data_utils\n",
    "import seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dbgannon/tensorflow-master/tensorflow/models/rnn/translate\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylearning_rate = 0.5\n",
    "mylearning_rate_decay_factor = 0.99\n",
    "mymax_gradient_norm = 5.0\n",
    "mybatch_size= 64\n",
    "mysize = 256\n",
    "mynum_layers = 2\n",
    "myen_vocab_size = 40000\n",
    "myfr_vocab_size = 40000\n",
    "mydata_dir = \"/datadrive/data\"\n",
    "mytrain_dir = \"/datadrive/tmp\"\n",
    "mymax_train_data_size = 0\n",
    "mysteps_per_checkpoint = 50\n",
    "mydecode = True\n",
    "myself_test = False\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(source_path, target_path, max_size=None):\n",
    "  \"\"\"Read data from source and target files and put into buckets.\n",
    "  Args:\n",
    "    source_path: path to the files with token-ids for the source language.\n",
    "    target_path: path to the file with token-ids for the target language;\n",
    "      it must be aligned with the source file: n-th line contains the desired\n",
    "      output for n-th line from the source_path.\n",
    "    max_size: maximum number of lines to read, all other will be ignored;\n",
    "      if 0 or None, data files will be read completely (no limit).\n",
    "  Returns:\n",
    "    data_set: a list of length len(_buckets); data_set[n] contains a list of\n",
    "      (source, target) pairs read from the provided data files that fit\n",
    "      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n",
    "      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n",
    "  \"\"\"\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      myen_vocab_size, myfr_vocab_size, _buckets,\n",
    "      mysize, mynum_layers, mymax_gradient_norm, mybatch_size,\n",
    "      mylearning_rate, mylearning_rate_decay_factor,\n",
    "      forward_only=forward_only)\n",
    "  ckpt = tf.train.get_checkpoint_state(mytrain_dir)\n",
    "  model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# Create model and load parameters.\n",
    "model = create_model(sess, True)\n",
    "model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabularies.\n",
    "en_vocab_path = os.path.join(mydata_dir,\n",
    "                             \"vocab%d.en\" % myen_vocab_size)\n",
    "fr_vocab_path = os.path.join(mydata_dir,\n",
    "                             \"vocab%d.fr\" % myfr_vocab_size)\n",
    "en_vocab, rev_en_vocab = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "_, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \" I am a person. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_ids = data_utils.sentence_to_token_ids(sentence, en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "bucket_id = min([b for b in xrange(len(_buckets))if _buckets[b][0] > len(token_ids)])\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if data_utils.EOS_ID in outputs:\n",
    "    outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis une personne .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([rev_fr_vocab[output] for output in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
